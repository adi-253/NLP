{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9588353,"sourceType":"datasetVersion","datasetId":5847616}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-09T18:01:35.648123Z","iopub.execute_input":"2024-10-09T18:01:35.648734Z","iopub.status.idle":"2024-10-09T18:01:36.428605Z","shell.execute_reply.started":"2024-10-09T18:01:35.648694Z","shell.execute_reply":"2024-10-09T18:01:36.427740Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/newscompany/tata_aspect.csv\n/kaggle/input/newscompany/apollo_aspect.csv\n/kaggle/input/newscompany/reliance_aspect.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ndevice=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-09T18:03:18.412817Z","iopub.execute_input":"2024-10-09T18:03:18.413425Z","iopub.status.idle":"2024-10-09T18:03:18.464739Z","shell.execute_reply.started":"2024-10-09T18:03:18.413384Z","shell.execute_reply":"2024-10-09T18:03:18.463705Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import AutoModel , AutoTokenizer,AutoConfig\nimport torch\nfrom torch.utils.data import Dataset,DataLoader\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom torch.optim import AdamW","metadata":{"execution":{"iopub.status.busy":"2024-10-09T18:28:11.172481Z","iopub.execute_input":"2024-10-09T18:28:11.173358Z","iopub.status.idle":"2024-10-09T18:28:12.632175Z","shell.execute_reply.started":"2024-10-09T18:28:11.173318Z","shell.execute_reply":"2024-10-09T18:28:12.631075Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/newscompany/reliance_aspect.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-09T18:29:01.833879Z","iopub.execute_input":"2024-10-09T18:29:01.834494Z","iopub.status.idle":"2024-10-09T18:29:01.915542Z","shell.execute_reply.started":"2024-10-09T18:29:01.834454Z","shell.execute_reply":"2024-10-09T18:29:01.914507Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T18:29:05.601754Z","iopub.execute_input":"2024-10-09T18:29:05.602635Z","iopub.status.idle":"2024-10-09T18:29:05.621621Z","shell.execute_reply.started":"2024-10-09T18:29:05.602562Z","shell.execute_reply":"2024-10-09T18:29:05.620629Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"         date                                               news  \\\n0  2017-12-18  So far, Reliance Industries has Hotstar and Ro...   \n1  2017-12-23  The rally in the countrys most valuable compan...   \n2  2017-12-24  Mukesh Ambani sets out five aims for Reliance ...   \n3  2017-12-25  Mukesh Ambani set five aims for the company an...   \n4  2018-01-02  The latest deal between the estranged Ambani b...   \n\n  Predicted Sentiment  Sentiment Probability  \\\n0             neutral               0.828784   \n1            positive               0.914790   \n2             neutral               0.826013   \n3             neutral               0.760601   \n4            positive               0.548908   \n\n                                    aspect_labelling  \n0  [[('stake', 'stock_market', 'neutral', 0.92604...  \n1  [[('Reliance', 'companies', 'positive', 0.9455...  \n2  [[('Reliance', 'companies', 'neutral', 0.58129...  \n3  [[('mukesh ambani', 'investor_sentiment', 'neu...  \n4  [[('Reliance', 'companies', 'neutral', 0.62014...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>news</th>\n      <th>Predicted Sentiment</th>\n      <th>Sentiment Probability</th>\n      <th>aspect_labelling</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-12-18</td>\n      <td>So far, Reliance Industries has Hotstar and Ro...</td>\n      <td>neutral</td>\n      <td>0.828784</td>\n      <td>[[('stake', 'stock_market', 'neutral', 0.92604...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-12-23</td>\n      <td>The rally in the countrys most valuable compan...</td>\n      <td>positive</td>\n      <td>0.914790</td>\n      <td>[[('Reliance', 'companies', 'positive', 0.9455...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-12-24</td>\n      <td>Mukesh Ambani sets out five aims for Reliance ...</td>\n      <td>neutral</td>\n      <td>0.826013</td>\n      <td>[[('Reliance', 'companies', 'neutral', 0.58129...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-12-25</td>\n      <td>Mukesh Ambani set five aims for the company an...</td>\n      <td>neutral</td>\n      <td>0.760601</td>\n      <td>[[('mukesh ambani', 'investor_sentiment', 'neu...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-01-02</td>\n      <td>The latest deal between the estranged Ambani b...</td>\n      <td>positive</td>\n      <td>0.548908</td>\n      <td>[[('Reliance', 'companies', 'neutral', 0.62014...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"   def extract_embeddings(self,features,inputs):\n        last_hidden_state=features[0]\n        #print(features.last_hidden_state.shape)\n        #print(features.keys())\n        #print(len(features['hidden_states']))\n        #print(features['last_hidden_state']==features[\"hidden_states\"][0])\n        #print(last_hidden_state[:,512:])\n        attention_mask=inputs[\"attention_mask\"]\n        embeddings=self.pool(last_hidden_state,attention_mask)\n        \n        return embeddings\n        \n    def forward(self,inputs):\n        features=self.model(**inputs)\n        out=self.extract_embeddings(features,inputs)\n        f1=self.fc1(out)\n        f1=self.relu(f1)\n        f1=self.layer_norm(f1)\n        f=self.fc2(f1)\n        return f\n        ","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(train_loader,model,optimizer,scheduler,epoch,gradient_accumulation_steps=2):\n    model.train()\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    steps=0\n    total_loss=0\n    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}\")\n    for step, (inputs, labels) in progress_bar:\n            inputs, labels = inputs.to(device), labels.to(device)\n            with torch.cuda.amp.autocast(enabled=True):\n                y_preds = model(inputs)\n                loss = loss_fn(y_preds, labels)\n            if gradient_accumulation_steps > 1:\n                loss = loss / gradient_accumulation_steps\n            \n            scaler.scale(loss).backward()\n            total_loss+=loss.item()\n            steps+=1\n            grad_norm = nn.utils.clip_grad_norm_(model.parameters(),100)\n            \n            if (step+1)%2==0:\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                scheduler.step()","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(text,tokenizer):\n    inputs=tokenizer.encode_plus(text,\n                                   add_special_tokens=True,\n                                   max_length=1467,\n                                   padding='max_length',\n                                   truncation=True)\n    #inputs={k:torch.tensor(v, dtype=torch.long).unsqueeze(0) for k,v in inputs.items()}\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class prepare_dataset(Dataset):\n    \n    def __init__(self,df):\n        self.text=df[\"full_text\"].values\n        self.labels=df[target_cols].values\n        \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self,idx):\n        inputs=prepare_input(self.text[idx],tokenizer)\n        #for k, v in inputs.items():\n            #print(f\"{k} shape:\", v.shape)\n        labels=self.labels[idx]\n        \n        return inputs,labels","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanPooling(nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self,last_hidden_state,attention_mask):\n        attention_mask_expanded=attention_mask.unsqueeze(-1).expand(last_hidden_state.shape).float()\n\n        sum_embeddings=torch.sum(attention_mask_expanded*last_hidden_state,1)\n        sum_mask=attention_mask_expanded.sum(1)\n        sum_mask=torch.clamp(sum_mask,min=1e-9)\n        mean=sum_embeddings/sum_mask\n        \n        return mean\n\nclass MaxPooling(nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self,last_hidden_state,attention_mask):\n        attention_mask_expanded=attention_mask.unsqueeze(-1).expand(last_hidden_state.shape).float()\n        embeddings=last_hidden_state.clone()\n        embeddings[attention_mask_expanded==0]=-1e4\n        max_embeddings,_=torch.max(embeddings,dim=1)\n        return max_embeddings\n\nclass MinPooling(nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self,last_hidden_state,attention_mask):\n        attention_mask_expanded=attention_mask.unsqueeze(-1).expand(last_hidden_state.shape).float()\n        embeddings=last_hidden_state.clone()\n        embeddings[attention_mask_expanded==0]=1e-4\n        min_embeddings,_=torch.min(embeddings,dim=1)\n        return min_embeddings\n    \n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=AutoModel.from_pretrained(\"ProsusAI/finbert\",output_hidden_states=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T18:57:01.162121Z","iopub.execute_input":"2024-10-09T18:57:01.163166Z","iopub.status.idle":"2024-10-09T18:57:03.707881Z","shell.execute_reply.started":"2024-10-09T18:57:01.163125Z","shell.execute_reply":"2024-10-09T18:57:03.707057Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4907cc14eaa9413db6159771a2ba8e63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97b911c2ed514f4d839a16b57cab02a2"}},"metadata":{}}]},{"cell_type":"code","source":"model.config","metadata":{"execution":{"iopub.status.busy":"2024-10-09T18:57:08.595475Z","iopub.execute_input":"2024-10-09T18:57:08.596184Z","iopub.status.idle":"2024-10-09T18:57:08.603361Z","shell.execute_reply.started":"2024-10-09T18:57:08.596129Z","shell.execute_reply":"2024-10-09T18:57:08.602535Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"BertConfig {\n  \"_name_or_path\": \"ProsusAI/finbert\",\n  \"architectures\": [\n    \"BertForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"positive\",\n    \"1\": \"negative\",\n    \"2\": \"neutral\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"negative\": 1,\n    \"neutral\": 2,\n    \"positive\": 0\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.45.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")","metadata":{"execution":{"iopub.status.busy":"2024-10-09T18:58:51.321336Z","iopub.execute_input":"2024-10-09T18:58:51.321766Z","iopub.status.idle":"2024-10-09T18:58:51.761330Z","shell.execute_reply.started":"2024-10-09T18:58:51.321725Z","shell.execute_reply":"2024-10-09T18:58:51.760582Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model.config","metadata":{"execution":{"iopub.status.busy":"2024-10-09T18:58:55.835149Z","iopub.execute_input":"2024-10-09T18:58:55.835553Z","iopub.status.idle":"2024-10-09T18:58:55.843022Z","shell.execute_reply.started":"2024-10-09T18:58:55.835514Z","shell.execute_reply":"2024-10-09T18:58:55.842096Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"BertConfig {\n  \"_name_or_path\": \"ProsusAI/finbert\",\n  \"architectures\": [\n    \"BertForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"positive\",\n    \"1\": \"negative\",\n    \"2\": \"neutral\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"negative\": 1,\n    \"neutral\": 2,\n    \"positive\": 0\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.45.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}"},"metadata":{}}]},{"cell_type":"code","source":"class Model(nn.Module):\n    \n    def __init__(self,pool='mean',training=True):\n        super().__init__()\n        self.model=AutoModel.from_pretrained(\"microsoft/deberta-v3-base\",output_hidden_states=True)\n        if training:\n            self.model.config.gradient_checkpointing=True\n        if pool==\"mean\":\n            self.pool=MeanPooling()\n        elif pool==\"max\":\n            self.pool=MaxPooling()\n        else:\n            self.pool=MeanPooling()\n            \n        if training:\n            for param in self.model.parameters():\n                param.requires_grad = False\n\n\n            for param in self.model.encoder.layer[-5:].parameters():\n                    param.requires_grad = True\n      \n        \n   \n    \n    def extract_embeddings(self,features,inputs):\n        last_hidden_state=features[0]\n        #print(features.last_hidden_state.shape)\n        #print(features.keys())\n        #print(len(features['hidden_states']))\n        #print(features['last_hidden_state']==features[\"hidden_states\"][0])\n        #print(last_hidden_state[:,512:])\n        attention_mask=inputs[\"attention_mask\"]\n        embeddings=self.pool(last_hidden_state,attention_mask)\n        \n        return embeddings\n        \n    def forward(self,inputs):\n        features=self.model(**inputs)\n        out=self.extract_embeddings(features,inputs)\n        return out\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}